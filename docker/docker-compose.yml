version: '3.8'
services:
  etl:
    build:
      context: ../
      dockerfile: docker/Dockerfile
    # volumes:
    #   - ./app:/usr/src/app
    command: python spark_etl.py
    env_file:
      - ../config/aws_credentials.env

    environment:
    - SPARK_HOME=/opt/spark
    - HADOOP_CONF_DIR=/opt/spark/conf
    - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
    - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    - REGION=${REGION}
    container_name: etl-container
    networks:
      - etl_network

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    ports:
      - "8082:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
    networks:
      - etl_network

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - etl_network

  airflow:
    image: apache/airflow:2.7.3
    container_name: airflow
    ports:
      - "8081:8080"
    environment:
      - LOAD_EXAMPLES=no
    volumes:
      - ./dags:/opt/airflow/dags
    networks:
      - etl_network

  postgres:
    image: postgres:latest
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    networks:
      - etl_network

networks:
  etl_network:
