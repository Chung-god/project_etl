version: '3.8'
services:
  etl:
    build:
      context: ../
      dockerfile: docker/Dockerfile
    # volumes:
    #   - ./app:/usr/src/app
    command: python spark_etl.py
    environment:
    - SPARK_HOME=/usr/local/spark
    - HADOOP_CONF_DIR=/usr/local/spark/conf
    - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
    - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    - REGION=${REGION}
    - POSTGRES_USER=etl_user
    - POSTGRES_PASSWORD=etl_password
    - POSTGRES_URL=jdbc:postgresql://postgres:5432/etl_db
    - POSTGRES_TABLE=processed_data
    container_name: etl-container
    networks:
      - etl_network

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    ports:
      - "8082:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
    networks:
      - etl_network

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - etl_network

  airflow:
    image: apache/airflow:2.7.3
    container_name: airflow
    ports:
      - "8081:8080"
    environment:
      - LOAD_EXAMPLES=no
    volumes:
      - ./dags:/opt/airflow/dags
    networks:
      - etl_network

  postgres:
    image: postgres:latest
    container_name: postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres_password
    ports:
      - "5432:5432"
    volumes:
      - ./docker/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - etl_network

networks:
  etl_network:
