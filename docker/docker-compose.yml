version: '3.8'

services:
  etl:
    build:
      context: ../
      dockerfile: docker/Dockerfile
    command: python spark_etl.py
    environment:
      - SPARK_HOME=/usr/local/spark
      - HADOOP_CONF_DIR=/usr/local/spark/conf
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - REGION=${REGION}
      - POSTGRES_USER=etl_user
      - POSTGRES_PASSWORD=etl_password
      - POSTGRES_URL=jdbc:postgresql://postgres:5432/etl_db
      - POSTGRES_TABLE=processed_data
    container_name: etl-container
    networks:
      - etl_network

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    ports:
      - "8082:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
    networks:
      - etl_network

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - etl_network

  airflow-init:
    image: apache/airflow:2.8.0
    container_name: airflow-init
    entrypoint: ["airflow", "db", "upgrade"]
    depends_on:
      - postgres
    environment:
      - AIRFLOW__WEBSERVER__DEFAULT_USERNAME=admin
      - AIRFLOW__WEBSERVER__DEFAULT_PASSWORD=admin
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://etl_user:etl_password@postgres:5432/etl_db
    networks:
      - etl_network

  airflow:
    image: apache/airflow:2.8.0
    container_name: airflow
    ports:
      - "8081:8080"
    environment:
      - AIRFLOW__WEBSERVER__DEFAULT_USERNAME=admin
      - AIRFLOW__WEBSERVER__DEFAULT_PASSWORD=admin
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://etl_user:etl_password@postgres:5432/etl_db
    volumes:
      - ./dags:/opt/airflow/dags
    depends_on:
      - airflow-init
    command: "airflow webserver"
    networks:
      - etl_network

  airflow-scheduler:
    image: apache/airflow:2.8.0
    container_name: airflow-scheduler
    depends_on:
      - airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__WEBSERVER__DEFAULT_USERNAME=admin
      - AIRFLOW__WEBSERVER__DEFAULT_PASSWORD=admin
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://etl_user:etl_password@postgres:5432/etl_db
    volumes:
      - ./dags:/opt/airflow/dags
    command: "airflow scheduler"
    networks:
      - etl_network


  postgres:
    image: postgres:latest
    container_name: postgres
    environment:
      POSTGRES_USER: etl_user
      POSTGRES_PASSWORD: etl_password
      POSTGRES_DB: etl_db
    ports:
      - "5432:5432"
    volumes:
      - ./docker/init.sql:/docker-entrypoint-initdb.d/init.sql
      - postgres_data:/var/lib/postgresql/data
    networks:
      - etl_network

  superset:
    image: apache/superset:latest
    container_name: superset
    environment:
      - SUPERSET_ENV=production
      - SUPERSET_ADMIN_USERNAME=admin
      - SUPERSET_ADMIN_PASSWORD=admin
      - SUPERSET_DB_HOST=postgres
      - SUPERSET_DB_PORT=5432
      - SUPERSET_DB_USER=etl_user
      - SUPERSET_DB_PASSWORD=etl_password
      - SUPERSET_DB_NAME=etl_db
      - SUPERSET_SECRET_KEY=wf7lkOsxpTr0ZCoeQTGOw7ese033U1P9LPDRhzR76zyK44uzM4geqK6q
    depends_on:
      - postgres
    ports:
      - "8088:8088"
    volumes:
      - superset_home:/app/superset_home
      - ./superset_config.py:/app/pythonpath/superset_config.py
    networks:
      - etl_network

networks:
  etl_network:
    driver: bridge

volumes:
  superset_home:
  postgres_data:
