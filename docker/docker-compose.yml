version: '3.8'
services:
  etl:
    build:
      context: ../
      dockerfile: docker/Dockerfile
    volumes:
      - ./app:/usr/src/app
    command: python spark_etl.py
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - REGION=${REGION}
    container_name: etl-container
  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077

  airflow:
    image: apache/airflow:2.7.3
    container_name: airflow
    ports:
      - "8081:8080"
    environment:
      - LOAD_EXAMPLES=no
    volumes:
      - ./dags:/opt/airflow/dags

  postgres:
    image: postgres:latest
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"

